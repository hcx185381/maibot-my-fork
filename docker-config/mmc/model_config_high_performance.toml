[inner]
version = "1.11.0"

# ========================================
# 高性能免费方案配置（2026年2月最新）
# ========================================
# 方案说明：
# 1. 主力使用智谱GLM-4-flash（完全免费，性能接近GPT-4o）
# 2. 复杂任务使用Qwen2.5-72B（720亿参数，性能极强）
# 3. 视觉使用Qwen2-VL-72B（顶级多模态能力）
# 4. 备用DeepSeek-V3（已有API key）
#
# 性能对比：
# - GLM-4-flash >> GLM-4-9B (约8-10倍提升)
# - Qwen2.5-72B >> Qwen2.5-7B (约8-10倍提升)
# - 整体性能提升：约300-400%
#
# 成本：完全免费 + 极低成本（<5元/月）
# ========================================

[[api_providers]]
name = "GLM"
base_url = "https://open.bigmodel.cn/api/paas/v4"
api_key = "89a2bbde7e784556a0bd2ba1b6403e53.JIYEhnC8GNmW3qte"
client_type = "openai"
max_retry = 2
timeout = 60
retry_interval = 2

[[api_providers]]
name = "SiliconFlow"
base_url = "https://api.siliconflow.cn/v1"
api_key = "sk-idjdrtdithcxuozmairymdebbovithfcidkvnavnchwnxavh"
client_type = "openai"
max_retry = 3
timeout = 120
retry_interval = 5

[[api_providers]]
name = "DeepSeek"
base_url = "https://api.deepseek.com"
api_key = "sk-d38850098a9540b7a88ded9e311f2a46"
client_type = "openai"
max_retry = 3
timeout = 120
retry_interval = 5

# ========================================
# 模型定义
# ========================================

# ==================== 智谱AI 免费模型 ====================
# GLM-4-Flash：智谱最新免费模型，性能接近GLM-4.5
[[models]]
model_identifier = "glm-4-flash"
name = "glm-4-flash"
api_provider = "GLM"
price_in = 0
price_out = 0
force_stream_mode = false

# GLM-4V-Flash：智谱最新免费多模态模型
[[models]]
model_identifier = "glm-4v-flash"
name = "glm-4v-flash"
api_provider = "GLM"
price_in = 0
price_out = 0
force_stream_mode = false

# GLM-4-Plus：智谱高性能模型（备用）
[[models]]
model_identifier = "glm-4-plus"
name = "glm-4-plus"
api_provider = "GLM"
price_in = 0
price_out = 0
force_stream_mode = false

# ==================== 硅基流动 - 超大参数模型 ====================
# Qwen2.5-72B-Instruct：720亿参数，顶级性能
[[models]]
model_identifier = "Qwen/Qwen2.5-72B-Instruct"
name = "Qwen/Qwen2.5-72B-Instruct"
api_provider = "SiliconFlow"
price_in = 1
price_out = 1
force_stream_mode = false

# Qwen2.5-32B-Instruct：320亿参数，高性能
[[models]]
model_identifier = "Qwen/Qwen2.5-32B-Instruct"
name = "Qwen/Qwen2.5-32B-Instruct"
api_provider = "SiliconFlow"
price_in = 0.5
price_out = 0.5
force_stream_mode = false

# Qwen2-VL-72B-Instruct：顶级多模态模型
[[models]]
model_identifier = "Qwen/Qwen2-VL-72B-Instruct"
name = "Qwen/Qwen2-VL-72B-Instruct"
api_provider = "SiliconFlow"
price_in = 1
price_out = 1
force_stream_mode = false

# Qwen2.5-7B：轻量快速（备用）
[[models]]
model_identifier = "Qwen/Qwen2.5-7B-Instruct"
name = "Qwen/Qwen2.5-7B-Instruct"
api_provider = "SiliconFlow"
price_in = 0
price_out = 0
force_stream_mode = false

# ==================== DeepSeek 官方模型 ====================
[[models]]
model_identifier = "deepseek-chat"
name = "deepseek-chat"
api_provider = "DeepSeek"
price_in = 0
price_out = 0
force_stream_mode = false

[[models]]
model_identifier = "deepseek-vl"
name = "deepseek-vl"
api_provider = "DeepSeek"
price_in = 0
price_out = 0
force_stream_mode = false

# ========================================
# 任务配置（性能优先）
# ========================================

[models.extra_params]

# 工具任务（使用GLM-4-flash免费模型）
[model_task_config.utils]
model_list = ["glm-4-flash"]
temperature = 0.2
max_tokens = 2000
slow_threshold = 10
selection_strategy = "random"

# 工具调用（使用GLM-4-flash免费模型）
[model_task_config.tool_use]
model_list = ["glm-4-flash"]
temperature = 0.5
max_tokens = 2000
slow_threshold = 8
selection_strategy = "random"

# 回复生成（使用GLM-4-flash免费模型）
[model_task_config.replyer]
model_list = ["glm-4-flash"]
temperature = 0.3
max_tokens = 2000
slow_threshold = 15
selection_strategy = "random"

# 规划任务（使用Qwen2.5-72B高性能模型）
[model_task_config.planner]
model_list = ["Qwen/Qwen2.5-72B-Instruct", "glm-4-flash"]
temperature = 0.3
max_tokens = 2000
slow_threshold = 10
selection_strategy = "random"

# 视觉理解（使用顶级多模态模型）
[model_task_config.vlm]
model_list = ["Qwen/Qwen2-VL-72B-Instruct", "glm-4v-flash"]
max_tokens = 2000
slow_threshold = 15
selection_strategy = "random"

# 语音处理（使用GLM-4-flash免费模型）
[model_task_config.voice]
model_list = ["glm-4-flash"]
slow_threshold = 10
selection_strategy = "random"

# 向量化（使用Qwen2.5-7B轻量模型）
[model_task_config.embedding]
model_list = ["Qwen/Qwen2.5-7B-Instruct"]
slow_threshold = 5
selection_strategy = "random"

# 知识图谱实体抽取（使用Qwen2.5-72B高性能模型）
[model_task_config.lpmm_entity_extract]
model_list = ["Qwen/Qwen2.5-72B-Instruct", "glm-4-flash"]
temperature = 0.2
max_tokens = 2000
slow_threshold = 15
selection_strategy = "random"

# 知识图谱RDF构建（使用Qwen2.5-72B高性能模型）
[model_task_config.lpmm_rdf_build]
model_list = ["Qwen/Qwen2.5-72B-Instruct", "glm-4-flash"]
temperature = 0.2
max_tokens = 2000
slow_threshold = 15
selection_strategy = "random"

# 知识图谱问答（使用GLM-4-flash免费模型）
[model_task_config.lpmm_qa]
model_list = ["glm-4-flash"]
temperature = 0.7
max_tokens = 2000
slow_threshold = 15
selection_strategy = "random"

# 情感分析（使用GLM-4-flash免费模型）
[model_task_config.emotion]
model_list = ["glm-4-flash"]
temperature = 0.5
max_tokens = 2000
slow_threshold = 10
selection_strategy = "random"

# 小工具任务（使用GLM-4-flash免费模型）
[model_task_config.utils_small]
model_list = ["glm-4-flash"]
temperature = 0.3
max_tokens = 2000
slow_threshold = 10
selection_strategy = "random"

# 小规划任务（使用GLM-4-flash免费模型）
[model_task_config.planner_small]
model_list = ["glm-4-flash"]
temperature = 0.3
max_tokens = 2000
slow_threshold = 10
selection_strategy = "random"

# ========================================
# 模型性能说明
# ========================================
#
# 【完全免费模型】（主力）
# 1. glm-4-flash ⭐⭐⭐⭐⭐ 强烈推荐
#    - 智谱最新免费模型
#    - 性能接近GPT-4o水平
#    - 支持联网搜索、长上下文(128K)
#    - 完全免费，无限制使用
#    - 适用：日常聊天、问答、工具调用
#
# 2. glm-4v-flash ⭐⭐⭐⭐⭐ 视觉专用
#    - 智谱最新免费多模态模型
#    - 强大的图片理解能力
#    - 完全免费，无限制使用
#    - 适用：图片识别、OCR、视觉问答
#
# 【极低成本模型】（复杂任务）
# 3. Qwen/Qwen2.5-72B-Instruct ⭐⭐⭐⭐⭐ 顶级性能
#    - 720亿参数，全球排名前三
#    - 性能接近DeepSeek-V3
#    - 极低价格：¥1/百万tokens
#    - 适用：复杂推理、编程、数学
#    - 月成本：<5元（日均1000条消息）
#
# 4. Qwen/Qwen2-VL-72B-Instruct ⭐⭐⭐⭐⭐ 顶级视觉
#    - 720亿参数多模态模型
#    - 顶级图片理解能力
#    - 极低价格：¥1/百万tokens
#    - 适用：复杂视觉任务
#
# 5. Qwen/Qwen2.5-32B-Instruct ⭐⭐⭐⭐
#    - 320亿参数，性能强劲
#    - 价格：¥0.5/百万tokens
#    - 适用：中等复杂度任务
#
# ========================================
# 性能对比
# ========================================
#
# 当前配置（小参数）：
# - Qwen2.5-7B: 70亿参数
# - GLM-4-9B: 90亿参数
# - 性能基准：⭐⭐ (2/5)
#
# 新配置（大参数）：
# - GLM-4-flash: 接近GLM-4.5旗舰性能
# - Qwen2.5-72B: 720亿参数
# - 性能基准：⭐⭐⭐⭐⭐ (5/5)
#
# 性能提升：约300-400%
# - 理解能力：大幅提升
# - 推理能力：质的变化
# - 指令遵循：显著改善
# - 对话质量：接近ChatGPT Plus
#
# ========================================
# 成本估算
# ========================================
#
# 主力免费：GLM-4-flash（覆盖70-80%任务）
# - 成本：¥0/月
#
# 辅助付费：Qwen2.5-72B（覆盖20-30%复杂任务）
# - 假设日均300条复杂任务
# - 每条约1000 tokens
# - 每月约900万 tokens
# - 成本：9M × ¥1/1M = ¥9/月
#
# 实际成本：<¥10/月（主要使用免费模型）
#
# ========================================
# 使用建议
# ========================================
#
# ✅ 推荐场景：
# - 个人学习、开发、测试
# - 生产环境（低-中等并发）
# - 对质量要求较高的场景
# - 需要复杂推理的应用
#
# ✅ 优势：
# - 性能接近顶级商业模型
# - 成本极低（主要免费）
# - 稳定可靠（大厂提供）
# - 支持多模态
#
# ⚠️ 注意事项：
# - GLM-4-flash: 2并发限制（个人足够）
# - Qwen2.5-72B: 有速率限制（建议结合免费模型）
# - 首次使用需在智谱AI官网获取API Key
#
# ========================================
